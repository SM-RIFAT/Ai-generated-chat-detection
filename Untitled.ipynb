{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95657cff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bnemo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Line2D\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbnemo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Translator\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01municodedata\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbnunicodenormalizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Normalizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bnemo'"
     ]
    }
   ],
   "source": [
    "# Regular EDA(exploratory data analysis) and plotting libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import statistics\n",
    "import re\n",
    "import joblib\n",
    "import random\n",
    "import statistics\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import os\n",
    "# import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import zipfile\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from bnemo import Translator\n",
    "import unicodedata\n",
    "from bnunicodenormalizer import Normalizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from datasets import Dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d09bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# from datasets import Dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4530bafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c199b8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>source</th>\n",
       "      <th>RDizzl3_seven</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phones\\n\\nModern humans today are always on th...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This essay will explain if drivers should or s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Driving while the use of cellular devices\\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phones &amp; Driving\\n\\nDrivers should not be able...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cell Phone Operation While Driving\\n\\nThe abil...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73568</th>\n",
       "      <td>I am writing you today to disagree with your t...</td>\n",
       "      <td>1</td>\n",
       "      <td>Grades for extracurricular activities</td>\n",
       "      <td>persuade_finetuned_llamas</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73569</th>\n",
       "      <td>Dear Principal,\\n\\nIn conclusion, I would obse...</td>\n",
       "      <td>1</td>\n",
       "      <td>Grades for extracurricular activities</td>\n",
       "      <td>persuade_finetuned_llamas</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73570</th>\n",
       "      <td>Dear Mrs. Principal,\\n\\nin these kinds of cons...</td>\n",
       "      <td>1</td>\n",
       "      <td>Grades for extracurricular activities</td>\n",
       "      <td>persuade_finetuned_llamas</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73571</th>\n",
       "      <td>I enjoyed Form five and excitedly ex claims ed...</td>\n",
       "      <td>1</td>\n",
       "      <td>Grades for extracurricular activities</td>\n",
       "      <td>persuade_finetuned_llamas</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73572</th>\n",
       "      <td>Dear TEACHER_NAME,\\n\\nWell Ms/Mr TEACHER_NAME ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Grades for extracurricular activities</td>\n",
       "      <td>persuade_finetuned_llamas</td>\n",
       "      <td>False</td>\n",
       "      <td>llama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73573 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0      Phones\\n\\nModern humans today are always on th...      0   \n",
       "1      This essay will explain if drivers should or s...      0   \n",
       "2      Driving while the use of cellular devices\\n\\nT...      0   \n",
       "3      Phones & Driving\\n\\nDrivers should not be able...      0   \n",
       "4      Cell Phone Operation While Driving\\n\\nThe abil...      0   \n",
       "...                                                  ...    ...   \n",
       "73568  I am writing you today to disagree with your t...      1   \n",
       "73569  Dear Principal,\\n\\nIn conclusion, I would obse...      1   \n",
       "73570  Dear Mrs. Principal,\\n\\nin these kinds of cons...      1   \n",
       "73571  I enjoyed Form five and excitedly ex claims ed...      1   \n",
       "73572  Dear TEACHER_NAME,\\n\\nWell Ms/Mr TEACHER_NAME ...      1   \n",
       "\n",
       "                                 prompt_name                     source  \\\n",
       "0                         Phones and driving            persuade_corpus   \n",
       "1                         Phones and driving            persuade_corpus   \n",
       "2                         Phones and driving            persuade_corpus   \n",
       "3                         Phones and driving            persuade_corpus   \n",
       "4                         Phones and driving            persuade_corpus   \n",
       "...                                      ...                        ...   \n",
       "73568  Grades for extracurricular activities  persuade_finetuned_llamas   \n",
       "73569  Grades for extracurricular activities  persuade_finetuned_llamas   \n",
       "73570  Grades for extracurricular activities  persuade_finetuned_llamas   \n",
       "73571  Grades for extracurricular activities  persuade_finetuned_llamas   \n",
       "73572  Grades for extracurricular activities  persuade_finetuned_llamas   \n",
       "\n",
       "       RDizzl3_seven  model  \n",
       "0              False  human  \n",
       "1              False  human  \n",
       "2              False  human  \n",
       "3              False  human  \n",
       "4              False  human  \n",
       "...              ...    ...  \n",
       "73568          False  llama  \n",
       "73569          False  llama  \n",
       "73570          False  llama  \n",
       "73571          False  llama  \n",
       "73572          False  llama  \n",
       "\n",
       "[73573 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train_v4_drcat_01.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6575316c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp = \"results/sent_model/\"\n",
    "model= AutoModelForSequenceClassification.from_pretrained(cp)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "968e2871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0aa77df1a04cd1ab40969ad66fdc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\golam\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\golam\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b6016b54da43e1b5d184b7a7378b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c42b2d042843d28139a37bb52674b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96be9f8a8cf422da98adbedc71848fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phones\n",
      "\n",
      "Modern humans today are always on their phone. They are always on their phone more than 5 hours a day no stop .All they do is text back and forward and just have group Chats on social media. They even do it while driving. They are some really bad consequences when stuff happens when it comes to a phone. Some certain areas in the United States ban phones from class rooms just because of it.\n",
      "\n",
      "When people have phones, they know about certain apps that they have .Apps like Facebook Twitter Instagram and Snapchat. So like if a friend moves away and you want to be in contact you can still be in contact by posting videos or text messages. People always have different ways how to communicate with a phone. Phones have changed due to our generation.\n",
      "\n",
      "Driving is one of the way how to get around. People always be on their phones while doing it. Which can cause serious Problems. That's why there's a thing that's called no texting while driving. That's a really important thing to remember. Some people still do it because they think It's stupid. No matter what they do they still have to obey it because that's the only way how did he save.\n",
      "\n",
      "Sometimes on the news there is either an accident or a suicide. It might involve someone not looking where they're going or tweet that someone sent. It either injury or death. If a mysterious number says I'm going to kill you and they know where you live but you don't know the person's contact ,It makes you puzzled and make you start to freak out. Which can end up really badly.\n",
      "\n",
      "Phones are fine to use and it's also the best way to come over help. If you go through a problem and you can't find help you ,always have a phone there with you. Even though phones are used almost every day as long as you're safe it would come into use if you get into trouble. Make sure you do not be like this phone while you're in the middle of driving. The news always updated when people do something stupid around that involves their phones. The safest way is the best way to stay safe.    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 11640,  2715,  4286,  2651,  2024,  2467,  2006,  2037,  3042,\n",
       "          1012,  2027,  2024,  2467,  2006,  2037,  3042,  2062,  2084,  1019,\n",
       "          2847,  1037,  2154,  2053,  2644,  1012,  2035,  2027,  2079,  2003,\n",
       "          3793,  2067,  1998,  2830,  1998,  2074,  2031,  2177, 11834,  2015,\n",
       "          2006,  2591,  2865,  1012,  2027,  2130,  2079,  2009,  2096,  4439,\n",
       "          1012,  2027,  2024,  2070,  2428,  2919,  8465,  2043,  4933,  6433,\n",
       "          2043,  2009,  3310,  2000,  1037,  3042,  1012,  2070,  3056,  2752,\n",
       "          1999,  1996,  2142,  2163,  7221, 11640,  2013,  2465,  4734,  2074,\n",
       "          2138,  1997,  2009,  1012,  2043,  2111,  2031, 11640,  1010,  2027,\n",
       "          2113,  2055,  3056, 18726,  2008,  2027,  2031,  1012, 18726,  2066,\n",
       "          9130, 10474, 16021, 23091,  1998, 10245,  7507,  2102,  1012,  2061,\n",
       "          2066,  2065,  1037,  2767,  5829,  2185,  1998,  2017,  2215,  2000,\n",
       "          2022,  1999,  3967,  2017,  2064,  2145,  2022,  1999,  3967,  2011,\n",
       "         14739,  6876,  2030,  3793,  7696,  1012,  2111,  2467,  2031,  2367,\n",
       "          3971,  2129,  2000, 10639,  2007,  1037,  3042,  1012, 11640,  2031,\n",
       "          2904,  2349,  2000,  2256,  4245,  1012,  4439,  2003,  2028,  1997,\n",
       "          1996,  2126,  2129,  2000,  2131,  2105,  1012,  2111,  2467,  2022,\n",
       "          2006,  2037, 11640,  2096,  2725,  2009,  1012,  2029,  2064,  3426,\n",
       "          3809,  3471,  1012,  2008,  1005,  1055,  2339,  2045,  1005,  1055,\n",
       "          1037,  2518,  2008,  1005,  1055,  2170,  2053,  3793,  2075,  2096,\n",
       "          4439,  1012,  2008,  1005,  1055,  1037,  2428,  2590,  2518,  2000,\n",
       "          3342,  1012,  2070,  2111,  2145,  2079,  2009,  2138,  2027,  2228,\n",
       "          2009,  1005,  1055,  5236,  1012,  2053,  3043,  2054,  2027,  2079,\n",
       "          2027,  2145,  2031,  2000, 15470,  2009,  2138,  2008,  1005,  1055,\n",
       "          1996,  2069,  2126,  2129,  2106,  2002,  3828,  1012,  2823,  2006,\n",
       "          1996,  2739,  2045,  2003,  2593,  2019,  4926,  2030,  1037,  5920,\n",
       "          1012,  2009,  2453,  9125,  2619,  2025,  2559,  2073,  2027,  1005,\n",
       "          2128,  2183,  2030,  1056, 28394,  2102,  2008,  2619,  2741,  1012,\n",
       "          2009,  2593,  4544,  2030,  2331,  1012,  2065,  1037,  8075,  2193,\n",
       "          2758,  1045,  1005,  1049,  2183,  2000,  3102,  2017,  1998,  2027,\n",
       "          2113,  2073,  2017,  2444,  2021,  2017,  2123,  1005,  1056,  2113,\n",
       "          1996,  2711,  1005,  1055,  3967,  1010,  2009,  3084,  2017, 14909,\n",
       "          1998,  2191,  2017,  2707,  2000, 11576,  2041,  1012,  2029,  2064,\n",
       "          2203,  2039,  2428,  6649,  1012, 11640,  2024,  2986,  2000,  2224,\n",
       "          1998,  2009,  1005,  1055,  2036,  1996,  2190,  2126,  2000,  2272,\n",
       "          2058,  2393,  1012,  2065,  2017,  2175,  2083,  1037,  3291,  1998,\n",
       "          2017,  2064,  1005,  1056,  2424,  2393,  2017,  1010,  2467,  2031,\n",
       "          1037,  3042,  2045,  2007,  2017,  1012,  2130,  2295, 11640,  2024,\n",
       "          2109,  2471,  2296,  2154,  2004,  2146,  2004,  2017,  1005,  2128,\n",
       "          3647,  2009,  2052,  2272,  2046,  2224,  2065,  2017,  2131,  2046,\n",
       "          4390,  1012,  2191,  2469,  2017,  2079,  2025,  2022,  2066,  2023,\n",
       "          3042,  2096,  2017,  1005,  2128,  1999,  1996,  2690,  1997,  4439,\n",
       "          1012,  1996,  2739,  2467,  7172,  2043,  2111,  2079,  2242,  5236,\n",
       "          2105,  2008,  7336,  2037, 11640,  1012,  1996,  3647,  3367,  2126,\n",
       "          2003,  1996,  2190,  2126,  2000,  2994,  3647,  1012,   102]],\n",
       "       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "text = data['text'][0]\n",
    "print(text)\n",
    "inputs = tokenizer(text, return_tensors = \"pt\",\n",
    "                   truncation=True, padding=True, max_length=512).to(device)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f98e8092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model = model.to(device)\n",
    "    pred = model(**inputs)\n",
    "    logits = pred.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c4114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
